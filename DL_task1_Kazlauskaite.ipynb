{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkKNPZFfBxolnbDT4M7liv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristina-26/DEEP-LEARNING-TASK-1-Kazlauskaite/blob/main/DL_task1_Kazlauskaite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Kristina Kazlauskaitė\n",
        "\n",
        "LSP: S2416112\n",
        "\n",
        "Data Science (full-time studies), group 1"
      ],
      "metadata": {
        "id": "jp1PJnVTEMfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uQw0HQdiECQj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 4  # 'background', 'aeroplane', 'sofa', 'dog'\n",
        "CLASSES = ['background', 'aeroplane', 'sofa', 'dog']\n",
        "COLORS = [(0, 0, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255)]  # For visualization\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 0.00005"
      ],
      "metadata": {
        "id": "Zhg4p2eDEQzA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3eVOyVLETDa",
        "outputId": "c8cbd22b-4051-465f-ace7-fba51056af3f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepLabv3 model with ResNet50 backbone for semantic segmentation\n",
        "class SimpleSegmentationModel(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.model = torchvision.models.segmentation.deeplabv3_resnet50(weights=torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "        self.model.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1) # classifier layer with our number of classes (4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)['out']\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # using the mean and SD of ImageNet dataset\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "kuXvDp65EUYQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_transform(mask):\n",
        "    mask = mask.resize((256, 256), resample=Image.NEAREST)  # resize mask safely\n",
        "    mask = np.array(mask)\n",
        "\n",
        "    # create a new mask with our classes\n",
        "    new_mask = np.zeros_like(mask)\n",
        "\n",
        "    # VOC class IDs (0-indexed):\n",
        "    # aeroplane: 1, sofa: 18, dog: 12, background: 0\n",
        "    if 1 in mask:\n",
        "        print(\"Found aeroplane in mask\")\n",
        "    if 18 in mask:\n",
        "        print(\"Found sofa in mask\")\n",
        "    if 12 in mask:\n",
        "        print(\"Found dog in mask\")\n",
        "\n",
        "    new_mask[mask == 1] = 1  # aeroplane\n",
        "    new_mask[mask == 18] = 2  # sofa\n",
        "    new_mask[mask == 12] = 3  # dog\n",
        "\n",
        "    return torch.tensor(new_mask, dtype=torch.long)"
      ],
      "metadata": {
        "id": "xNmgaEboEVWb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter VOC to only have the selected classes (aeroplane, sofa and dog)\n",
        "class VOCMultiClassFiltered(Dataset):\n",
        "    def __init__(self, root, year='2012', image_set='train', transform=None, target_transform=None):\n",
        "        self.voc = VOCSegmentation(root=root, year=year, image_set=image_set, download=True) # root: root directory,\n",
        "        # year: VOC dataset year, image_set: 'train'/ 'val'\n",
        "        self.transform = transform # image transformations\n",
        "        self.target_transform = target_transform # mask transformations\n",
        "\n",
        "        # checking if masks contain these classes\n",
        "        self.valid_indices = []\n",
        "        print(\"Checking for valid images with classes of interest\")\n",
        "        for idx in range(min(3000, len(self.voc))):  # checking not more than 3000 images\n",
        "            _, target = self.voc[idx]\n",
        "            mask = np.array(target)\n",
        "            # looking for aeroplane (0), sofa (17), or dog (11)\n",
        "            if 1 in mask or 18 in mask or 12 in mask:\n",
        "                self.valid_indices.append(idx)\n",
        "                if len(self.valid_indices) % 10 == 0: # log message every 10th time valid image is found\n",
        "                    print(f\"Found {len(self.valid_indices)} valid images so far\")\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} images with classes of interest\")\n",
        "\n",
        "        # class distribution in valid images\n",
        "        class_counts = {1: 0, 18: 0, 12: 0}\n",
        "        for idx in self.valid_indices:\n",
        "            _, target = self.voc[idx]\n",
        "            mask = np.array(target)\n",
        "            for class_id in class_counts.keys():\n",
        "                if class_id in mask:\n",
        "                    class_counts[class_id] += 1\n",
        "\n",
        "        print(\"Class distribution in valid images:\")\n",
        "        print(f\"Aeroplane (1): {class_counts[1]} images\")\n",
        "        print(f\"Sofa (18): {class_counts[18]} images\")\n",
        "        print(f\"Dog (12): {class_counts[12]} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = self.voc[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "i6gHq7TFEWd6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VOCMultiClassFiltered(root='./data', transform=transform, target_transform=mask_transform)\n",
        "\n",
        "# use valid indices that contain our classes, or take a subset if we have enough\n",
        "if len(dataset.valid_indices) > 500:\n",
        "    # at least 1000 images\n",
        "    subset_size = min(1000, len(dataset.valid_indices))\n",
        "    subset = Subset(dataset, dataset.valid_indices[:subset_size])\n",
        "else:\n",
        "    subset = Subset(dataset, range(1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXJVQGLVEXn1",
        "outputId": "700e3122-9e50-4784-c544-2ac2f8bb7d5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for valid images with classes of interest\n",
            "Found 10 valid images so far\n",
            "Found 20 valid images so far\n",
            "Found 30 valid images so far\n",
            "Found 40 valid images so far\n",
            "Found 50 valid images so far\n",
            "Found 60 valid images so far\n",
            "Found 70 valid images so far\n",
            "Found 80 valid images so far\n",
            "Found 90 valid images so far\n",
            "Found 100 valid images so far\n",
            "Found 110 valid images so far\n",
            "Found 120 valid images so far\n",
            "Found 130 valid images so far\n",
            "Found 140 valid images so far\n",
            "Found 150 valid images so far\n",
            "Found 160 valid images so far\n",
            "Found 170 valid images so far\n",
            "Found 180 valid images so far\n",
            "Found 190 valid images so far\n",
            "Found 200 valid images so far\n",
            "Found 210 valid images so far\n",
            "Found 220 valid images so far\n",
            "Found 230 valid images so far\n",
            "Found 240 valid images so far\n",
            "Found 250 valid images so far\n",
            "Found 260 valid images so far\n",
            "Found 270 valid images so far\n",
            "Found 280 valid images so far\n",
            "Found 290 valid images so far\n",
            "Found 291 images with classes of interest\n",
            "Class distribution in valid images:\n",
            "Aeroplane (1): 88 images\n",
            "Sofa (18): 93 images\n",
            "Dog (12): 121 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(subset))\n",
        "val_size = len(subset) - train_size\n",
        "train_dataset, val_dataset = random_split(subset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "ji0vNmMVEZ5X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkorWSXVFjjS",
        "outputId": "52e1a816-daf9-4ffd-fc97-4baff7468945"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleSegmentationModel().to(device)\n",
        "\n",
        "# add class weighting to handle class imbalance (background usually dominates)\n",
        "# gives more importance to the minority classes\n",
        "class_weights = torch.tensor([0.1, 1.5, 1.5, 1.5]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "hQm2DMaxEbyt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training with validation after each epoch\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "patience = 3  # number of epochs to wait after validation loss and train loss divergence before stopping\n",
        "early_stop_counter = 0\n",
        "diverging = False\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # checking if masks contain our classes\n",
        "        mask_classes = torch.unique(masks)\n",
        "        if len(mask_classes) == 1 and mask_classes[0] == 0:\n",
        "            print(\"Warning: This batch only contains background class\")\n",
        "            continue  # skip batch if it only contains background\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # store losses for tracking divergence\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # saving the model if it's the best so far\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_segmentation_model.pth')\n",
        "        print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
        "        early_stop_counter = 0  # Reset counter when we find a better model\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print(f\"Validation loss did not improve for {early_stop_counter} epoch(s)\")\n",
        "\n",
        "    # sheck for divergence (train loss decreasing while val loss increasing)\n",
        "    if len(train_losses) >= 2:\n",
        "        if train_losses[-1] < train_losses[-2] and val_losses[-1] > val_losses[-2]:\n",
        "            diverging = True\n",
        "            print(f\"Warning: Training and validation losses are diverging (possible overfitting)\")\n",
        "        else:\n",
        "            diverging = False\n",
        "\n",
        "    # early stopping condition\n",
        "    if diverging and early_stop_counter >= patience:\n",
        "        print(f\"Early stopping triggered after epoch {epoch+1} due to loss divergence\")\n",
        "        print(f\"Using best model with validation loss: {best_val_loss:.4f}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RJXsa2lwEc4P",
        "outputId": "e8541841-e430-49c9-a51d-8117e5951992"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   0%|          | 0/400 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found sofa in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   0%|          | 1/400 [00:11<1:13:50, 11.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   1%|          | 3/400 [00:22<47:42,  7.21s/it]  "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   1%|▏         | 5/400 [00:34<42:14,  6.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   2%|▏         | 6/400 [00:44<49:58,  7.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   2%|▏         | 8/400 [00:55<42:56,  6.57s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found aeroplane in mask\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:   4%|▎         | 14/400 [01:06<18:26,  2.87s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n",
            "Found aeroplane in mask\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:   5%|▍         | 19/400 [01:30<23:40,  3.73s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   6%|▌         | 22/400 [01:41<23:30,  3.73s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:   6%|▌         | 24/400 [01:52<25:56,  4.14s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n",
            "Found sofa in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:   8%|▊         | 31/400 [02:04<15:03,  2.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n",
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:   9%|▉         | 36/400 [02:25<19:09,  3.16s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:  11%|█         | 43/400 [02:36<13:01,  2.19s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  12%|█▏        | 48/400 [02:36<08:14,  1.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  13%|█▎        | 53/400 [02:48<09:50,  1.70s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  14%|█▎        | 54/400 [02:59<14:37,  2.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  14%|█▍        | 55/400 [03:10<20:09,  3.51s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found dog in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  14%|█▍        | 56/400 [03:21<26:15,  4.58s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found sofa in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  14%|█▍        | 57/400 [03:32<31:59,  5.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  15%|█▍        | 59/400 [03:42<31:03,  5.46s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:  17%|█▋        | 68/400 [03:54<12:35,  2.28s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  18%|█▊        | 73/400 [04:05<12:22,  2.27s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  19%|█▉        | 75/400 [04:16<15:34,  2.88s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  19%|█▉        | 77/400 [04:27<18:25,  3.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  20%|█▉        | 79/400 [04:39<21:42,  4.06s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  20%|██        | 82/400 [04:50<20:47,  3.92s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/15 - Training:  21%|██        | 83/400 [05:00<24:57,  4.72s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Training:  23%|██▎       | 91/400 [05:11<11:55,  2.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  24%|██▍       | 95/400 [05:22<12:39,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  25%|██▍       | 99/400 [05:34<12:59,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15 - Training:  27%|██▋       | 107/400 [05:45<08:40,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  28%|██▊       | 111/400 [05:56<09:57,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found dog in mask\n",
            "Found aeroplane in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  28%|██▊       | 113/400 [06:07<12:50,  2.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found sofa in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  28%|██▊       | 114/400 [06:17<16:57,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: This batch only contains background class\n",
            "Warning: This batch only contains background class\n",
            "Found sofa in mask\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/15 - Training:  29%|██▉       | 116/400 [06:18<15:26,  3.26s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-23482443e657>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip batch if it only contains background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-ad22e19cf280>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m transform = transforms.Compose([\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/segmentation/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# contract: features is a dict of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model for evaluation\n",
        "model.load_state_dict(torch.load('best_segmentation_model.pth'))"
      ],
      "metadata": {
        "id": "Uh-OODB0Ed5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    class_correct = [0] * num_classes\n",
        "    class_total = [0] * num_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # calculate per-class accuracy\n",
        "            for c in range(num_classes):\n",
        "                class_mask = (masks == c)\n",
        "                class_correct[c] += (preds[class_mask] == c).sum().item()\n",
        "                class_total[c] += class_mask.sum().item()\n",
        "\n",
        "            preds = preds.cpu().numpy()\n",
        "            masks = masks.cpu().numpy()\n",
        "            all_preds.extend(preds.flatten())\n",
        "            all_labels.extend(masks.flatten())\n",
        "\n",
        "    # per-class accuracy\n",
        "    class_accuracy = []\n",
        "    for i in range(num_classes):\n",
        "        if class_total[i] > 0:\n",
        "            accuracy = class_correct[i] / class_total[i]\n",
        "            class_accuracy.append(accuracy)\n",
        "            print(f\"Class {CLASSES[i]} accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            class_accuracy.append(0)\n",
        "            print(f\"Class {CLASSES[i]} has no samples\")\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'precision': precision_score(all_labels, all_preds, average='weighted', labels=range(num_classes), zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, average='weighted', labels=range(num_classes), zero_division=0),\n",
        "        'f1': f1_score(all_labels, all_preds, average='weighted', labels=range(num_classes), zero_division=0),\n",
        "        'confusion_matrix': confusion_matrix(all_labels, all_preds, labels=range(num_classes)),\n",
        "        'class_accuracy': class_accuracy\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Z6MdzvJXEf11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate_model(model, val_loader, device, NUM_CLASSES)\n",
        "\n",
        "print(\"\\nModel Performance Metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    if k != 'confusion_matrix' and k != 'class_accuracy':\n",
        "        print(f\"{k.capitalize()}: {v:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(metrics['confusion_matrix'])"
      ],
      "metadata": {
        "id": "azIBlpmHEhKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model, dataset, device, num_samples=3):\n",
        "    model.eval()\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    for idx in indices:\n",
        "        image, true_mask = dataset[idx]\n",
        "        image_batch = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_batch)\n",
        "            pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "        # de-normalize image for visualization\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "        def decode_mask(mask):\n",
        "            color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
        "            for class_id, color in enumerate(COLORS):\n",
        "                color_mask[mask == class_id] = color\n",
        "            return color_mask\n",
        "\n",
        "        # print unique class values for debugging\n",
        "        true_classes = np.unique(true_mask.numpy())\n",
        "        pred_classes = np.unique(pred_mask)\n",
        "        print(f\"True mask classes: {true_classes}\")\n",
        "        print(f\"Predicted mask classes: {pred_classes}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(decode_mask(true_mask.numpy()))\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(decode_mask(pred_mask))\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OxRp4502EiTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVisualizing predictions:\")\n",
        "visualize_predictions(model, val_dataset, device, num_samples=3)"
      ],
      "metadata": {
        "id": "U_T-k4TbEkcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone"
      ],
      "metadata": {
        "id": "5PD6tZi-Elay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# download OpenImages\n",
        "print(\"Downloading OpenImages with class labels...\")\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"open-images-v6\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"], # to get ground truth class labels\n",
        "    classes=[\"Dog\", \"Sofa bed\", \"Airplane\"],\n",
        "    max_samples=100,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# keep track of which classes exist in each image\n",
        "class OpenImagesClassDataset(Dataset):\n",
        "    def __init__(self, fo_dataset, transform=None):\n",
        "        self.samples = []\n",
        "        self.class_mapping = {\"Airplane\": 1, \"Sofa bed\": 2, \"Dog\": 3}\n",
        "        self.transform = transform\n",
        "\n",
        "        # get ground truth class labels\n",
        "        for sample in fo_dataset:\n",
        "            image_path = sample.filepath\n",
        "\n",
        "            gt_classes = set()\n",
        "            if hasattr(sample, \"ground_truth\") and sample.ground_truth is not None:\n",
        "                if hasattr(sample.ground_truth, \"detections\") and sample.ground_truth.detections is not None:\n",
        "                    for detection in sample.ground_truth.detections:\n",
        "                        class_name = detection.label\n",
        "                        if class_name in self.class_mapping:\n",
        "                            gt_classes.add(self.class_mapping[class_name])\n",
        "\n",
        "            # store sample data\n",
        "            self.samples.append({\n",
        "                'path': image_path,\n",
        "                'gt_classes': gt_classes,\n",
        "                'filename': os.path.basename(image_path)\n",
        "            })\n",
        "\n",
        "        # images per class\n",
        "        class_counts = {1: 0, 2: 0, 3: 0}\n",
        "        for sample in self.samples:\n",
        "            for class_id in sample['gt_classes']:\n",
        "                class_counts[class_id] += 1\n",
        "\n",
        "        print(f\"Processed {len(self.samples)} images with ground truth class labels\")\n",
        "        print(f\"Class distribution in dataset:\")\n",
        "        print(f\"- Airplane: {class_counts[1]} images\")\n",
        "        print(f\"- Sofa bed: {class_counts[2]} images\")\n",
        "        print(f\"- Dog: {class_counts[3]} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image_path = sample['path']\n",
        "        gt_classes = sample['gt_classes']\n",
        "\n",
        "        # load and process image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize((256, 256), Image.BILINEAR)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # convert ground truth to multi-hot encoding\n",
        "        gt_labels = np.zeros(4)  # [background, airplane, sofa bed, dog]\n",
        "        for class_id in gt_classes:\n",
        "            gt_labels[class_id] = 1\n",
        "\n",
        "        return image, torch.tensor(gt_labels), sample['filename']\n",
        "\n",
        "# dataset and dataloader\n",
        "openimages_dataset = OpenImagesClassDataset(dataset, transform=transform)\n",
        "openimages_loader = DataLoader(openimages_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 4. Evaluate model's class detection performance\n",
        "def evaluate_class_detection(model, dataloader, device, num_classes=4):\n",
        "    model.eval()\n",
        "\n",
        "    # storage for predictions and ground truth\n",
        "    all_preds = []\n",
        "    all_gt = []\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, gt_labels, filename in dataloader:\n",
        "            image = image.to(device)\n",
        "\n",
        "            # run model\n",
        "            output = model(image)\n",
        "            pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "            # convert segmentation to class detection (if any pixel is predicted as class X, then class X is detected)\n",
        "            pred_classes = np.zeros(num_classes)\n",
        "            for class_id in range(num_classes):\n",
        "                if np.any(pred_mask == class_id):\n",
        "                    pred_classes[class_id] = 1\n",
        "\n",
        "            all_preds.append(pred_classes)\n",
        "            all_gt.append(gt_labels.numpy().squeeze())\n",
        "\n",
        "            # Store detailed result\n",
        "            results.append({\n",
        "                'filename': filename[0],\n",
        "                'predicted_classes': [i for i in range(num_classes) if pred_classes[i] == 1],\n",
        "                'ground_truth_classes': [i for i in range(num_classes) if gt_labels[0, i] == 1]\n",
        "            })\n",
        "\n",
        "    # convert to np arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_gt = np.array(all_gt)\n",
        "\n",
        "    # per-class metrics\n",
        "    per_class_metrics = []\n",
        "    class_names = ['background', 'airplane', 'sofa bed', 'dog']\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        class_preds = all_preds[:, c]\n",
        "        class_gt = all_gt[:, c]\n",
        "\n",
        "        # calculate metrics (handle case where a class might not be present)\n",
        "        if np.any(class_gt):\n",
        "            accuracy = accuracy_score(class_gt, class_preds)\n",
        "            precision = precision_score(class_gt, class_preds, zero_division=0)\n",
        "            recall = recall_score(class_gt, class_preds, zero_division=0)\n",
        "            f1 = f1_score(class_gt, class_preds, zero_division=0)\n",
        "        else:\n",
        "            accuracy, precision, recall, f1 = 0, 0, 0, 0\n",
        "\n",
        "        per_class_metrics.append({\n",
        "            'class': class_names[c],\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "    # aggregate metrics\n",
        "    aggregate_metrics = {\n",
        "        'accuracy': accuracy_score(all_gt.flatten(), all_preds.flatten()),\n",
        "        'precision': precision_score(all_gt.flatten(), all_preds.flatten(), zero_division=0),\n",
        "        'recall': recall_score(all_gt.flatten(), all_preds.flatten(), zero_division=0),\n",
        "        'f1': f1_score(all_gt.flatten(), all_preds.flatten(), zero_division=0)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'per_class': per_class_metrics,\n",
        "        'aggregate': aggregate_metrics,\n",
        "        'details': results\n",
        "    }"
      ],
      "metadata": {
        "id": "M1tzfWs_Em4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  evaluation\n",
        "print(\"\\nEvaluating class detection on OpenImages:\")\n",
        "metrics = evaluate_class_detection(model, openimages_loader, device)\n",
        "\n",
        "# display results\n",
        "print(\"\\nAggregate Metrics:\")\n",
        "for metric, value in metrics['aggregate'].items():\n",
        "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nPer-Class Metrics:\")\n",
        "for class_metrics in metrics['per_class']:\n",
        "    class_name = class_metrics['class']\n",
        "    print(f\"\\nClass: {class_name}\")\n",
        "    for metric, value in class_metrics.items():\n",
        "        if metric != 'class':\n",
        "            print(f\"  {metric.capitalize()}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "p5wYMyIMExTT",
        "outputId": "5f4af04a-a525-4aa4-ddb5-a66a16e46985"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating class detection on OpenImages:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_class_detection' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-237acd711343>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating class detection on OpenImages:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_class_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenimages_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_class_detection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize some examples\n",
        "def visualize_class_detection_results(model, dataloader, device, num_samples=5):\n",
        "    model.eval()\n",
        "    class_names = ['background', 'airplane', 'sofa bed', 'dog']\n",
        "\n",
        "    samples = []\n",
        "    for data in dataloader:\n",
        "        samples.append(data)\n",
        "        if len(samples) >= num_samples:\n",
        "            break\n",
        "\n",
        "    for image, gt_labels, filename in samples:\n",
        "        image = image.to(device)\n",
        "\n",
        "        output = model(image)\n",
        "        pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "        pred_classes = []\n",
        "        for class_id in range(1, 4):  # skip background\n",
        "            if np.any(pred_mask == class_id):\n",
        "                pred_classes.append(class_names[class_id])\n",
        "\n",
        "        # get ground truth classes\n",
        "        gt_classes = []\n",
        "        for class_id in range(1, 4):\n",
        "            if gt_labels[0, class_id] == 1:\n",
        "                gt_classes.append(class_names[class_id])\n",
        "\n",
        "        img_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "        # colorize segmentation\n",
        "        color_mask = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)\n",
        "        for class_id, color in enumerate(COLORS):\n",
        "            color_mask[pred_mask == class_id] = color\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(img_np)\n",
        "        plt.title(f\"Image: {filename[0]}\\nGround Truth Classes: {', '.join(gt_classes)}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(color_mask)\n",
        "        plt.title(f\"Segmentation\\nPredicted Classes: {', '.join(pred_classes)}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        match = set(gt_classes) == set(pred_classes)\n",
        "        plt.suptitle(f\"Class Detection: {'Correct' if match else '✗ Incorrect'}\",\n",
        "                     color='green' if match else 'red', fontsize=16)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "f7AiSAtlEzhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVisualizing class detection results:\")\n",
        "visualize_class_detection_results(model, openimages_loader, device)"
      ],
      "metadata": {
        "id": "45jmi8nVE_ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q opencv-python matplotlib\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "id": "n_bov1DUFA-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# load SAM model\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "2PpsFlnoFDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_with_sam(dataset, num_samples=10, model=None, device=None, compute_metrics=True):\n",
        "    if model is None or device is None:\n",
        "        print(\"Model and device must be provided\")\n",
        "        return\n",
        "\n",
        "    # storage for metrics\n",
        "    metrics = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': [],\n",
        "        'iou': []\n",
        "    }\n",
        "\n",
        "    # get random indices\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    # track average metrics\n",
        "    all_metrics = []\n",
        "\n",
        "    for idx in indices:\n",
        "        image, _, filename = dataset[idx]\n",
        "\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "        image_bgr = (image_np[..., ::-1] * 255).astype(np.uint8)  # RGB → BGR for OpenCV\n",
        "\n",
        "        # SAM prediction\n",
        "        predictor.set_image(image_bgr)\n",
        "        input_point = np.array([[128, 128]])  # center point\n",
        "        input_label = np.array([1])\n",
        "        masks, _, _ = predictor.predict(\n",
        "            point_coords=input_point,\n",
        "            point_labels=input_label,\n",
        "            multimask_output=True\n",
        "        )\n",
        "        sam_mask = masks[0]  # first mask\n",
        "\n",
        "        # our model prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(image.unsqueeze(0).to(device))\n",
        "            pred_mask = torch.argmax(pred.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "        # convert multi-class prediction to binary (foreground/background), non-background (class 0) pixel is considered foreground (1)\n",
        "        our_binary_mask = (pred_mask > 0).astype(np.uint8)\n",
        "\n",
        "        # convert SAM mask to binary\n",
        "        sam_binary_mask = sam_mask.astype(np.uint8)\n",
        "\n",
        "        # metrics between our model and SAM\n",
        "        if compute_metrics:\n",
        "            # flatten masks\n",
        "            our_flat = our_binary_mask.flatten()\n",
        "            sam_flat = sam_binary_mask.flatten()\n",
        "\n",
        "            # metrics\n",
        "            accuracy = accuracy_score(sam_flat, our_flat)\n",
        "            precision = precision_score(sam_flat, our_flat, zero_division=0)\n",
        "            recall = recall_score(sam_flat, our_flat, zero_division=0)\n",
        "            f1 = f1_score(sam_flat, our_flat, zero_division=0)\n",
        "\n",
        "            # IoU\n",
        "            intersection = np.logical_and(our_binary_mask, sam_binary_mask).sum()\n",
        "            union = np.logical_or(our_binary_mask, sam_binary_mask).sum()\n",
        "            iou = intersection / union if union > 0 else 0\n",
        "\n",
        "            # metrics\n",
        "            metrics['accuracy'].append(accuracy)\n",
        "            metrics['precision'].append(precision)\n",
        "            metrics['recall'].append(recall)\n",
        "            metrics['f1'].append(f1)\n",
        "            metrics['iou'].append(iou)\n",
        "\n",
        "            all_metrics.append({\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'iou': iou\n",
        "            })\n",
        "\n",
        "        # visualize\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(f\"Image: {filename}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(sam_binary_mask, cmap='gray')\n",
        "        plt.title(\"Segment Anything Mask\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "\n",
        "        decoded = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)\n",
        "        for class_id, color in enumerate(COLORS):\n",
        "            decoded[pred_mask == class_id] = color\n",
        "        plt.imshow(decoded)\n",
        "\n",
        "        # add metrics to the plot title if available\n",
        "        if compute_metrics and all_metrics:\n",
        "            curr_metrics = all_metrics[-1]\n",
        "            plt.title(f\"Model\\nF1: {curr_metrics['f1']:.3f}, IoU: {curr_metrics['iou']:.3f}\")\n",
        "        else:\n",
        "            plt.title(\"Model Prediction\")\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if compute_metrics and metrics['accuracy']:\n",
        "        avg_metrics = {metric: np.mean(values) for metric, values in metrics.items()}\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nComparison Metrics between our model and SAM:\")\n",
        "        print(f\"Accuracy:  {avg_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Precision: {avg_metrics['precision']:.4f}\")\n",
        "        print(f\"Recall:    {avg_metrics['recall']:.4f}\")\n",
        "        print(f\"F1 Score:  {avg_metrics['f1']:.4f}\")\n",
        "        print(f\"IoU:       {avg_metrics['iou']:.4f}\")\n",
        "\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "Q4roh2R0FG2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## import metrics\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# comparison\n",
        "print(\"Comparing the model with SAM on OpenImages:\")\n",
        "comparison_metrics = compare_with_sam(openimages_dataset, num_samples=10, model=model, device=device)"
      ],
      "metadata": {
        "id": "r3TmRufQFJ0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# define class names and colors\n",
        "CLASSES = ['background', 'aeroplane', 'sofa', 'dog']\n",
        "COLORS = [(0, 0, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "\n",
        "# function to process all images in a folder\n",
        "def process_test_folder(model, device, sam_predictor=None, folder_path=\"test\"):\n",
        "    # check if folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
        "        print(\"Please upload the folder.\")\n",
        "        return None\n",
        "\n",
        "    # get all image files from the folder\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
        "    image_files = []\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        ext = os.path.splitext(file)[1].lower()\n",
        "        if ext in image_extensions:\n",
        "            image_files.append(os.path.join(folder_path, file))\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No image files found in folder '{folder_path}'\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found {len(image_files)} images in the folder. Processing...\")\n",
        "\n",
        "    # summary metrics for SAM comparison (if available)\n",
        "    if sam_predictor is not None:\n",
        "        sam_metrics = {\n",
        "            'iou': [],\n",
        "            'accuracy': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'f1': []\n",
        "        }\n",
        "\n",
        "    # process each image\n",
        "    for i, image_path in enumerate(image_files):\n",
        "        print(f\"\\nProcessing image {i+1}/{len(image_files)}: {os.path.basename(image_path)}\")\n",
        "\n",
        "        try:\n",
        "            # load the image\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            # show the uploaded image\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(image)\n",
        "            plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # define the same transforms as used during training\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            # apply transforms\n",
        "            image_tensor = transform(image).unsqueeze(0)  # add batch dimension\n",
        "            image_tensor = image_tensor.to(device)\n",
        "\n",
        "            # run inference with our model\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = model(image_tensor)\n",
        "                pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
        "\n",
        "            # process for visualization\n",
        "            image_resized = image.resize((256, 256))\n",
        "            img_np = np.array(image_resized) / 255.0\n",
        "\n",
        "            # create colored segmentation mask\n",
        "            color_mask = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)\n",
        "            for class_id, color in enumerate(COLORS):\n",
        "                color_mask[pred_mask == class_id] = color\n",
        "\n",
        "            # count pixels per class and calculate percentages\n",
        "            class_pixels = {}\n",
        "            total_pixels = pred_mask.size\n",
        "            for idx, class_name in enumerate(CLASSES):\n",
        "                count = np.sum(pred_mask == idx)\n",
        "                percentage = 100 * count / total_pixels\n",
        "                class_pixels[class_name] = f\"{percentage:.1f}%\"\n",
        "\n",
        "            # determine the dominant class (excluding background)\n",
        "            dominant_class = None\n",
        "            max_percent = 0\n",
        "            for class_name, percentage in class_pixels.items():\n",
        "                percent_value = float(percentage.strip('%'))\n",
        "                if class_name != 'background' and percent_value > max_percent:\n",
        "                    max_percent = percent_value\n",
        "                    dominant_class = class_name\n",
        "\n",
        "            if sam_predictor is not None:\n",
        "                # prepare image for SAM\n",
        "                sam_image = img_np.copy()\n",
        "                sam_image = (sam_image * 255).astype(np.uint8)\n",
        "\n",
        "                # set image for SAM\n",
        "                sam_predictor.set_image(sam_image)\n",
        "\n",
        "                # generate mask using center point\n",
        "                input_point = np.array([[128, 128]])  # center point\n",
        "                input_label = np.array([1])  # foreground\n",
        "\n",
        "                masks, _, _ = sam_predictor.predict(\n",
        "                    point_coords=input_point,\n",
        "                    point_labels=input_label,\n",
        "                    multimask_output=True\n",
        "                )\n",
        "\n",
        "                # best mask (first one)\n",
        "                sam_mask = masks[0]\n",
        "\n",
        "                # metrics between our model and SAM\n",
        "                your_binary = (pred_mask > 0).astype(np.uint8)  # Any non-background is 1\n",
        "\n",
        "                # flatten masks for metrics\n",
        "                your_flat = your_binary.flatten()\n",
        "                sam_flat = sam_mask.flatten()\n",
        "\n",
        "                # metrics\n",
        "                accuracy = accuracy_score(sam_flat, your_flat)\n",
        "                precision = precision_score(sam_flat, your_flat, zero_division=0)\n",
        "                recall = recall_score(sam_flat, your_flat, zero_division=0)\n",
        "                f1 = f1_score(sam_flat, your_flat, zero_division=0)\n",
        "\n",
        "                # IoU\n",
        "                intersection = np.logical_and(your_binary, sam_mask).sum()\n",
        "                union = np.logical_or(your_binary, sam_mask).sum()\n",
        "                iou = intersection / union if union > 0 else 0\n",
        "\n",
        "                # store metrics\n",
        "                sam_metrics['accuracy'].append(accuracy)\n",
        "                sam_metrics['precision'].append(precision)\n",
        "                sam_metrics['recall'].append(recall)\n",
        "                sam_metrics['f1'].append(f1)\n",
        "                sam_metrics['iou'].append(iou)\n",
        "\n",
        "                # visualizations with comparison\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # original image\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(img_np)\n",
        "                plt.title(f\"Original: {os.path.basename(image_path)}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # model prediction\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(color_mask)\n",
        "                plt.title(f\"Our Model\\n{dominant_class or 'Background'}: {class_pixels[dominant_class or 'background']}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # SAM prediction\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(sam_mask, cmap='gray')\n",
        "                plt.title(f\"SAM\\nIoU: {iou:.3f}, F1: {f1:.3f}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.suptitle(f\"Model Comparison - Image {i+1}/{len(image_files)}\", fontsize=14)\n",
        "                plt.tight_layout()\n",
        "                plt.subplots_adjust(top=0.85)\n",
        "                plt.show()\n",
        "\n",
        "                # Print metrics\n",
        "                print(f\"\\nMetrics comparing our model with SAM:\")\n",
        "                print(f\"IoU: {iou:.4f}\")\n",
        "                print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                print(f\"Precision: {precision:.4f}\")\n",
        "                print(f\"Recall: {recall:.4f}\")\n",
        "                print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "            else:\n",
        "                plt.figure(figsize=(12, 4))\n",
        "\n",
        "                # original image\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(img_np)\n",
        "                plt.title(f\"Original: {os.path.basename(image_path)}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # segmentation mask\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(color_mask)\n",
        "                plt.title(\"Segmentation\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # blend of image and mask\n",
        "                plt.subplot(1, 3, 3)\n",
        "                blend = img_np.copy()\n",
        "                # semi-transparent blend\n",
        "                for i in range(3):\n",
        "                    blend[:,:,i] = blend[:,:,i] * 0.5 + color_mask[:,:,i] / 255.0 * 0.5\n",
        "                plt.imshow(blend)\n",
        "                plt.title(\"Overlay\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.suptitle(f\"Segmentation Results\\n\" +\n",
        "                            \" | \".join([f\"{cls}: {pct}\" for cls, pct in class_pixels.items()]),\n",
        "                            fontsize=12)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.subplots_adjust(top=0.8)\n",
        "                plt.show()\n",
        "\n",
        "            # class percentages\n",
        "            print(\"\\nClass Distribution:\")\n",
        "            for class_name, percentage in class_pixels.items():\n",
        "                print(f\"- {class_name}: {percentage}\")\n",
        "\n",
        "            if dominant_class:\n",
        "                print(f\"\\nDominant class: {dominant_class} ({class_pixels[dominant_class]})\")\n",
        "            else:\n",
        "                print(\"\\nNo dominant class detected besides background\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image '{image_path}': {e}\")\n",
        "\n",
        "    # summary metrics if SAM comparison was performed\n",
        "    if sam_predictor is not None and sam_metrics['iou']:\n",
        "        # average metrics\n",
        "        avg_metrics = {metric: np.mean(values) for metric, values in sam_metrics.items()}\n",
        "\n",
        "        # summary\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SUMMARY: COMPARISON WITH SAM ACROSS ALL IMAGES\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Average IoU:       {avg_metrics['iou']:.4f}\")\n",
        "        print(f\"Average Accuracy:  {avg_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Average Precision: {avg_metrics['precision']:.4f}\")\n",
        "        print(f\"Average Recall:    {avg_metrics['recall']:.4f}\")\n",
        "        print(f\"Average F1 Score:  {avg_metrics['f1']:.4f}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "_W3lAx-GFKUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_test_folder(model, device, folder_path=\"test\")"
      ],
      "metadata": {
        "id": "uCYf_RJQFT-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with SAM\n",
        "process_test_folder(model, device, sam_predictor=predictor, folder_path=\"test\")"
      ],
      "metadata": {
        "id": "YAeyAcunFUyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}